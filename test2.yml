base_model: JackFram/llama-68m

load_in_8bit: true

datasets:
  - path: arcee-ai/distilabel-intel-orca-dpo-pairs-binarized
    type: chatml.ultra
    split: train
output_dir: ./outputs/lora-out

sequence_len: 1024

adapter: lora
lora_r: 64
lora_alpha: 32
lora_dropout: 0.1
lora_target_linear: true

rl: dpo
dpo_use_weighting: true

wandb_project: check_dpotrainer
wandb_entity: axolotl-ai
wandb_watch:
wandb_name: baseline/dpo_base/dpo_use_weighting
wandb_log_model:


num_epochs: 1
micro_batch_size: 4
gradient_accumulation_steps: 1
learning_rate: 0.00001
optimizer: paged_adamw_8bit
lr_scheduler: cosine
max_steps": 20
save_steps: 10
warmup_steps: 5
gradient_checkpointing: True
gradient_checkpointing_kwargs:
  use_reentrant: false
#special_tokens:
#  pad_token: <|end_of_text|>

